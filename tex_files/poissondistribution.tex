\section{Poisson Distribution}
\label{sec:poisson-distribution}

In this section we provide a derivation of, and motivation for, the
Poisson process, and clarify its relation with the exponential
distribution at the end.

Consider a machine that fails occasionally. Let us write $N(s, t)$ for
the number of failures occurring during a time interval of $(s,t]$. We
assume, without loss of generality, that repairs are
instantaneous. Clearly, as we do not know in advance how often the
machine will fail, we model $N(s,t)$ as a random variable for all
times $s$ and $t$.

Our first assumption is that the failure behavior of the machine does not
significantly change over time. Then it is reasonable to assume that
the expected number of failure is proportional to the  length of
the interval $T$. Thus, it is reasonable to assume that there exists some
constant $\lambda$ such that
\begin{equation}
  \label{eq:6}
 \E{N(s,t)} = \lambda (t-s)
\end{equation}
The constant $\lambda$ is often called the \recall{arrival rate}, or
failure rate in this case.


The second assumption is that $\{N(s,t), s\leq t\}$ has
\recall{stationary} and \emph{independent increments}. Stationarity
means that the distribution of the number of arrivals are the same for
all intervals of equal length. Formally, $N(s_1,t_1)$ has the same
distribution as $N(s_2, t_2)$ if $t_2-s_2 = t_1-s_1$. Independence
means, roughly speaking, that knowing that $N(s_1,t_1)= n$, does not
help to make any predictions about $N(s_2, t_2)$ if the intervals
$(s_1,t_1)$ and $(s_2, t_2)$ have no overlap.

To find the distribution of $N(0,t)$, let us split the interval
$[0,t]$ into $n$ sub-intervals, all of equal length, and ask: `What is
the probability that the machine will fail in some given
sub-interval.'  By our second assumption, the failure behavior is
constant over time. Therefore, the probability $p$ to fail in each
interval should be equal. Moreover, if $n$ is large, $p$ must be
small, for otherwise \eqref{eq:6} could not be true. As a consequence,
if the time intervals are very small, we can safely neglect the
probability that two or more failures occur in one such tiny interval.

As a consequence, then, we can model the occurrence of a failure in
some period $i$ as a Bernoulli distributed random variable $B_i$ such
that $\P{B_i =1}$ and $\P{B_i=0} = 1-\P{B_i = 1}$, and we assume that
$\{B_i\}$ are independent. The total number of failures $N_n(t)$ that
occur in $n$ intervals is then binomially distributed
\begin{equation}\label{eq:bin}
  \P{N_n(t) = k} = {n \choose k} p^k (1-p)^{n-k}.
\end{equation}
In the exercises we ask you to use this to motivate that, in some
appropriate sense, $N_n(t)$ converges to $N(t)$ for $n\to \infty$ such
that
\begin{equation}\label{eq:pois}
  \P{N(t) = k} = 
e^{-\lambda t} \frac{(\lambda t)^k}{k!}.
\end{equation}
We say that $N(t)$ is \recall{Poisson distributed} with rate
$\lambda$, and write $N(t)\sim P(\lambda t)$. 

Moreover, if there are no failures in some interval $[0,t]$, then it
must be that $N(t) = 0$ and the interarrival time $X_1=A_1-0$ (as
$A_0=0$) must be larger than $t$.  Therefore,
\begin{equation*}
 \P{X_1> t} = \P{N(t) = 0} = e^{-\lambda t} \frac{(\lambda t)^0}{0!}= e^{-\lambda t}.
\end{equation*}

The relations we discussed above are of paramount importance in the
analysis of queueing process. We summarize this by a theorem.
\begin{theorem}\label{thr:1}
  A counting process $\{N(t)\}$ is a Poisson process with rate $\lambda$ if and only if
  the inter-arrival times $\{X_i\}$, i.e., the times between consecutive
  arrivals, are i.i.d. and $\P{ X_1\leq t} = 1 - e^{-\lambda t}$. In other words, $X_i\sim \exp(\lambda) \Leftrightarrow N(t) \sim P(\lambda t)$ 
\end{theorem}

\begin{question}
Show that $\E{N_n(t)} = \sum_{i=1}^n \E{B_i} = n p$.
\hint{Recall that $\E{X+Y} = \E X + \E Y$. Just as a reminder, $\E{XY} \neq \E X \E Y$ in general. Only when $X$ and $Y$ are uncorrelated (which is implied by independence), the product of the expectations is the expection of the products.}
\begin{solution}
  \begin{equation*}
    \E{N_n(t)} = \E{\sum_{i=1}^n {B_i}} = \sum_{i=1}^n \E{B_i} = n \E{B_i} = n p.
  \end{equation*}
\end{solution}
\end{question}

\begin{question}
What is the difference between $N_n(t)$ and $N(t)$?
\begin{solution}
  $N_n(t)$ is a binomially distributed random variable with parameters
  $n$ and $p$. The maximum value of $N_n(t)$ is $n$. The random
  variable $N(t)$ models the number of failures that can occur during
  $[0,t]$. As such it is not necessarily bounded by $n$. Thus, $N_n(t)$ and $N(t)$ cannot represent the same random variable. 
\end{solution}
\end{question}

\begin{question}
  Show how the binomial distribution~\eqref{eq:bin} converges to the
  Poisson distribution~\eqref{eq:pois} if $n\to\infty$, $p\to0$ such
  that $np=\lambda$. 

  \hint{First find $p$, $n$, $\lambda$ and $t$ are such that the rate
    at which event occur in both processes are the same. Then consider
    the binomial distribution and use the standard limit
    $(1-x/n)^n \to e^{-x}$ as $n\to \infty$. }

  \begin{solution} Now we like to relate $N_n(t)$ and $N(t)$. It is
    clear that we at least want the expectations to be the same, that
    is, $np = \lambda t$. This implies that
\begin{equation*}
  p = \frac{\lambda t}n,
\end{equation*}
so that 
\begin{equation*}
  \P{N_n(t) = k} = {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k}.
\end{equation*}
To see that 
\begin{equation*}\label{eq:52}
  \lim_{n\to\infty} {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k} = e^{-\lambda t} \frac{(\lambda t)^k}{k!}.
\end{equation*}
use that
    \begin{equation*}
      \begin{split}
      {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k} 
&= \frac{n!}{k!(n-k)!} \left(\frac{\lambda t}{n}\frac{n}{n-\lambda t}\right)^k \left(1-\frac{\lambda t}n\right)^{n} \\
&= \frac{(\lambda t)^k}{k!} \left(\frac n{n-\lambda t} \right)^k  \frac{n!}{n^k(n-k)!}\left(1-\frac{\lambda t}n\right)^{n}\\
&= \frac{(\lambda t)^k}{k!} \left(\frac n{n-\lambda t} \right)^k \frac{n}{n}\frac{n-1}{n}\cdots\frac{n-k+1}{n} \left(1-\frac{\lambda t}n\right)^{n}.
\end{split}
\end{equation*}
Observe now that, as $\lambda t$ is finite, $n/(n-\lambda t)\to 1$ as
$n\to \infty$. Also for any finite $k$, $(n-k)/n\to1$. Finally, we use
that $(1+x/n)^n\to e^{x}$ so that ,
$\left(1-\frac{\lambda t}n\right)^{n} \to e^{-\lambda t}$.  The rest
is easy, so that, as $n\to\infty$,  the above converges to 
\begin{equation*}
\frac{(\lambda t)^k}{k!} e^{-\lambda t}.
\end{equation*}

  \end{solution}
\end{question}


\begin{question}
  If the inter-arrival times $\{X_i\}$ are i.i.d. and exponentially
  distributed with mean $1/\lambda$, prove that the number $N(t)$ of
  arrivals during interval $[0,t]$ is Poisson distributed.
  \hint{Realize that
    $\P{N(t)=k} = \P{A_k \leq t} - \P{A_{k+1} \leq t}$.}
    \begin{solution}
      We want to show that
    \begin{equation*}
      \P{N(t)=k} = e^{-\lambda t}\frac{(\lambda t)^k}{k!}.
    \end{equation*}
    Now observe that
    $\P{N(t)=k} = \P{A_k \leq t} - \P{A_{k+1} \leq t}$.  Using the
    density of $A_{k+1}$ as obtained previously and applying partial
    integration leads to
\begin{equation*}
  \begin{split}
\P{A_{k+1} \leq t} 
&= \lambda \int_0^t \frac{(\lambda s)^{k}}{k!}e^{-\lambda s}\, \d s \\
&= \lambda \frac{(\lambda s)^{k}}{k!}\frac{e^{-\lambda s}}{-\lambda} \Big|_{0}^t + \lambda \int_0^t \frac{(\lambda s)^{k-1}}{(k-1)!}e^{-\lambda s}\, \d s \\
&= - \frac{(\lambda t)^{k}}{k!} e^{-\lambda t} + \P{A_k \leq t}
  \end{split}
\end{equation*}
We are done.
    \end{solution}
\end{question}



\begin{question}
  Show that if $N(t) \sim P(\lambda t)$, we have for small $h$,
  \begin{enumerate}
  \item $\P{N(h) = n \given N(0) = n} = 1-\lambda h + o(h)$ 
  \item $\P{N(h) = n+1 \given N(0) = n} = \lambda h + o(h)$ 
  \item $\P{N(h) \geq n+2 \given N(0) = n} = o(h)$,
  \end{enumerate}
 where $o(h)$ is a function
    $f(h)$ such $f(h)/h \to 0$ as $h\to 0$.

    \hint{Think about the meaning of the formula
      $\P{N(h) = n \given N(0) = n}$. It is a conditional probability
      that should be read like this: given that up to time $0$ we have
      seen $n$ arrivals (i.e, $N(0)=n$), what is the probability that
      just a little later ($h$) the number of arrivals is still $n$,
      i.e, $N(h)=n$? Then use the definition of the Poisson
      distribtuion to compute this probability. Finally, use Taylor's
      expension of $e^{x}$ to see that $e^{x} = 1 +x + o(x)$ for
      $|x| << 1$.  Furthermore, use that
      $\sum_{i=2}^\infty x^i/i! = \sum_{i=0}^\infty x^i/i! - x -1 = e^x -x 
      - 1$.}

\begin{solution}

\begin{enumerate}
  \item 
  $\P{N(h) = n | N(0) = n} = \P{N(h) = 0} = e^{-\lambda h} (\lambda
  h)^0/0! = e^{-\lambda h} = 1-\lambda h + o(h)$,
  as follows from a standard argument in analysis that
  $e^{-\lambda h} = 1 -\lambda h + o(h)$  for $h$ small. 
\item 
  $\P{N(h) = n +1 | N(0) = n} = \P{N(h) = 1} = e^{-\lambda h} (\lambda h)^1/1! = 
(1-\lambda h + o(h))\lambda h  = \lambda h - \lambda^2 h^2 + o(h) = \lambda h + o(h)$. 
\item 
  \begin{equation*}
    \begin{split}
  \P{N(h) \geq n+2 | N(0) = n} 
&= \P{N(h) \geq 2} \\
&= e^{-\lambda h} \sum_{i=2}^\infty \frac{(\lambda h)^i}{i!} 
= e^{-\lambda h} \left(\sum_{i=0}^\infty \frac{(\lambda h)^i}{i!} - \lambda h - 1\right)\\
&= e^{-\lambda h}(e^{\lambda h} - 1 - \lambda h) 
= 1 - e^{-\lambda h}(1 + \lambda h) \\
&= 1 - (1-\lambda h + o(h))(1+\lambda h) 
= 1 - (1-\lambda^2 h^2 + o(h)) \\
&= \lambda^2 h^2 + o(h) = o(h).
    \end{split}
  \end{equation*}
\end{enumerate}
\end{solution}
\end{question}


\begin{question}
  Assume a timer fires at times $0=T_0<T_1<T_2< \cdots$, such that
  $T_{k}-T_{k-1}\sim\exp(\lambda)$. Define
  $N(t) = \sum_{k=0}^\infty k \1{T_k \leq t < T_{k+1}}$, where $\1{}$
  is the \recall{indicator function}, that is, $\1{A}=1$ if the event
  $A$ is true, and $\1{A}=0$ if $A$ is not true. What is the
  distribution of $N(t)$?
\begin{solution}
$N(t) \sim \text{P}(\lambda t)$. 
\end{solution}
\end{question}

\begin{question}(\tbd I added this question to clarify the next question, you can skip it for year 16/17.)
Show that  for a Poisson process $N$, 
\begin{equation*}
  \P{N(0,s] =1\given N(0,t]=1} = \frac st.
\end{equation*}
Thus, if you know that $N(0,t]=1$, the arrival is distributed
uniformly on the interval $[0,t]$.


\hint{Use Bayes' law for conditional probability. Observe that 
  \begin{equation*}
\{N(0,s]+N(s,t]=1\}\cap\{N(0,s]=1\} = \{1+N(s,t]=1\}\cap\{N(0,s]=1\}=\{N(s,t]=0\}\cap\{N(0,s]=1\}.
  \end{equation*}
}

\begin{solution}
\begin{equation*}
  \begin{split}
  \P{N(0,s] =1\given N(0,t]=1} 
&= \frac{\P{N(0,s] =1, N(0,t]=1}}{\P{N(0,t]=1}} \\
&=   \P{N(0,t] =1\given N(0,s]=1} \frac{\P{N(0,s] =1}}{\P{N(0,t]=1}} \\
&=   \P{N(0,s] + N(s,t] =1\given N(0,s]=1} \frac{e^{-\lambda s}\lambda s}{e^{-\lambda t} \lambda t} \\
&=   \P{1 + N(s,t] =1\given N(0,s]=1} e^{-\lambda (s-t)} \frac{s}{t} \\
&=   \P{N(s,t] =0} e^{-\lambda (s-t)} \frac{s}{t} = e^{-\lambda(t-s)} e^{-\lambda (s-t)} \frac{s}{t} = \frac st.
  \end{split}
\end{equation*}
\end{solution}
\end{question}


\begin{question} (Merged Poisson streams form a new Poisson process
  with the sum of the rates.) Assume that
  $N_a(t)\sim \text{P}(\lambda t)$ , $N_s(t) \sim \text{P}(\mu t)$ and
  independent.  Show that
  $N_a(t) + N_s(t) \sim \text{P}((\lambda + \mu)t)$. The figure below
  provides a graphical representation of merging (also called
  superposition) of streams.

  \begin{center}
\begin{tikzpicture}[scale=1]
%\draw[[-{Triangle[open]},dotted] (0,10)--(8.5,10);

\draw[->] (0,2)--(10,2);
\node[left] at (0,2) {$N_\lambda(t)$};
\draw[->] (0,1)--(10,1);
\node[left] at (0,1) {$N_\mu(t)$};
\draw[->] (0,0)--(10,0);
\node[left] at (0,0) {$N_{\lambda+\mu}(t)$};

\draw[{Rays[]}-{Rays[]},dotted] (1,2.06)--(1,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (1.5,1.06)--(1.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (3.2,2.06)--(3.2,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (3.5,1.06)--(3.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (4.5,1.06)--(4.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (5,1.06)--(5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (6.1,1.06)--(6.1,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (7.1,2.06)--(7.1,-0.06);

\end{tikzpicture}
    
  \end{center}



  \hint{Use a conditioning argument or use probability generating
    functions (i.e. $\E{z^X}=\sum_k z^k \P{X=k}$. In particular, for
    conditioning, use that $\P{AB} = \P{A\given B}\P{B}$. More
    generally, if the set $A$ can be split into disjoint sets $B_i$,
    i.e, $A=\bigcup_{i=1}^n B_i$, then
    \begin{equation*}
      \P{A} = \sum_{i=1}^n \P{A B_i} = \sum_{i=1}^n \P{A\given B_i} \P{B_i},
    \end{equation*}
    where we use the conditioning formula to see that
    $\P{A B_i} = \P{A \given B_i} \P{B_i}$.  Now choose practical sets
    $B_i$.  }

    \begin{solution}
First we show how to  use conditioning. 
  \begin{equation*}
    \begin{split}
\P{N_a(t) + N_s(t) = n} 
&= \sum_{i=0}^n \P{N_a(t) + N_s(t) = n|N_a(t) = i}\P{N_a(t)=i} \\
\end{split}
\end{equation*}
Now, if $N_a(t)=i$, then 
\begin{equation*}
N_a(t)+N_s(t) = n \iff 
i+N_s(t) = n \iff 
N_s(t) = n-i.
\end{equation*}
Thus,
  \begin{equation}\label{eq:002}
    \begin{split}
\P{N_a(t) + N_s(t) = n} 
&= \sum_{i=0}^n \P{ N_s(t) = n-i}\P{N_a(t)=i} \\
&= \sum_{i=0}^n \frac{(\mu t)^{n-i}}{(n-i)!} \frac{(\lambda t)^i}{i!} e^{-(\mu+\lambda)t} \\
&= e^{-(\mu+\lambda)t} \sum_{i=0}^n \frac{(\mu t)^{n-i}}{(n-i)!} \frac{(\lambda t)^i}{i!}  \\
&= e^{-(\mu+\lambda)t}\frac 1{n!} \sum_{i=0}^n {n \choose i }(\mu t)^{n-i}(\lambda t)^i  \\
&= \frac{((\mu+\lambda)t))^n}{n!}e^{-(\mu+\lambda)t}.
    \end{split}
  \end{equation}

Now   with the probability generating functions.
      \begin{equation*}
        \begin{split}
          M_a(z) 
&= \E{z^{N_a(t)}} = \sum_{k=0}^\infty z^k \P{N_a(t) = k} \\
&= \sum_{k=0}^\infty z^k e^{-\lambda t} \frac{(\lambda t)^k}{k!} \\
&= e^{-\lambda t} \sum_{k=0}^\infty  \frac{(z \lambda t)^k}{k!} \\
&=\exp(\lambda t (z-1)). 
        \end{split}
      \end{equation*}
      Use this, and the similar expression for $N_s(t)$ and
      independence to see that
\begin{equation*}
M(z) = \E{z^{N(t)}} = \E{z^{N_a(t)}} \E{z^{N_s(t)}} = \exp((\lambda +\lambda)t (z-1)). 
\end{equation*}
Finally, since the  generating function uniquely characterizes
the distribution, and since the above expression has the same form as
$M_1(z)$ but with $\lambda+\mu$ replacing $\lambda$, we can conclude that $N(t)\sim P((\lambda +\mu) t)$.
    \end{solution}
\end{question}


\begin{question} Assume that
  $N_a(t)\sim \text{P}(\lambda t)$ , $N_s(t) \sim \text{P}(\mu t)$ and
  independent. Use that  $N_a(t) + N_s(t) \sim \text{P}((\lambda + \mu)t)$ to conclude
    \begin{equation*}
    \P{N_a(h) = 1,  N_s(h) = 0 | N_a(h) + N_s(h) = 1} =
\frac{\lambda}{\lambda+\mu}.
    \end{equation*}
    Note that the right-hand-side does not depend on $h$, hence it
    holds for any time $h$, whether it is small or not.  \hint{Suppose
      we write $N(t)=N_a(t) + N_s(t)$. Then
      \begin{equation*}
      \P{N_a(h) = 1, N_s(h) = 0 | N(h) = 1}
      \end{equation*}
      is the probability that $N_a(h)=1$ and $N_s(h)=0$ \emph{given}
      that $N(t)=1$. In other words, the question is find out that,
      given one of the two processes `fired', what is the probability
      that $N_a$ was the one that `fired'.}

    \begin{solution}
  With the above:
  \begin{equation*}
    \begin{split}
&    \P{N_a(h) = 1,  N_s(h) = 0 | N_a(h) + N_s(h) = 1} \\
&= \frac{\P{N_a(h) = 1,  N_s(h) = 0, N_a(h) + N_s(h) = 1} }{ \P{N_a(h) + N_s(h) = 1}} \\ 
&= \frac{\P{N_a(h) = 1,  N_s(h) = 0}}{ \P{N_a(h) + N_s(h) = 1}} \\ 
&= \frac{\P{N_a(h) = 1}\P{N_s(h) = 0}}{ \P{N_a(h) + N_s(h) = 1}} \\ 
&= \frac{\lambda h \exp{-\lambda h} \exp{-\mu h}}{((\lambda+\mu)h)\exp{(-(\lambda+\mu)h)}}\\
&= \frac{\lambda h \exp{(-(\lambda + \mu)h)}}{((\lambda+\mu)h)\exp{(-(\lambda+\mu)h)}}\\
&= \frac{\lambda}{\lambda+\mu}
    \end{split}
  \end{equation*}
    \end{solution}
\end{question}


\begin{question}
 Assume that $N_a(t)\sim \text{P}(\lambda t)$ ,
  $N_s(t) \sim \text{P}(\mu t)$ and independent.  What is actually the
  meaning of the event
  $\{N_a(h) = 1, N_s(h) = 0\}\cap\{N_a(h) + N_s(h) = 1\}$?
    \begin{solution}
 This means that, given that an event occurred, the event was an arrival, i.e., $N_a$ was the first.
    \end{solution}
\end{question}


\begin{question} (A Bernoulli-thinned Poisson process is still a
  Poisson process) Consider a Poisson process. Split the process in
  the following way.  When a job arrives, throw a coin that lands
  heads with probability $p$ and tails with $q=1-p$. When the coin
  lands heads, call the job of type 1, otherwise of type 2.  Another
  way of thinning is by modeling the stream of people passing a shop
  as a Poisson process with rate $\lambda$. With probability $p$ a
  person decides, independent of anything else, to enter the shop,
  c.f. the figure below. The crosses at the upper line are passersby
  in the street. The crosses at the lower line are the customers that
  enter the shop. The outcome of the $k$th throw of a coin is
  indicated by $B_k$. Since $B_1=1$ the first passerby turns into a
  customer entering the shop; the second passerby does not enter as
  $B_2=0$, and so on. Like this, the stream of passerby's is thinned
  with Bernoulli distributed random variables.

  \begin{center}
\begin{tikzpicture}[scale=1]
%\draw[[-{Triangle[open]},dotted] (0,10)--(8.5,10);
\draw[->] (0,2)--(10,2);
\node[left] at (0,2) {$N_\lambda(t)$};
%\draw[->] (0,1)--(10,1);
%\node[left] at (0,1) {$B_k$};
\draw[->] (0,0)--(10,0);
\node[left] at (0,0) {$N_{\lambda p}(t)$};

\draw[{Rays[]}-{Rays[]},dotted] (1,2.06)--(1,-0.06) 
node[below]  {$B_1=1$};

\draw[{Rays[]}-{Circle[open]},dotted] (2.5,2.06)--(2.5,1.3) 
node[below] {$B_2=0$};

\draw[{Rays[]}-{Circle[open]},dotted] (4,2.06)--(4,1.3) 
node[below, fill=white] {$B_3=0$};

\draw[{Rays[]}-{Rays[]},dotted] (5,2.06)--(5,-0.06) 
node[below]  {$B_4=1$};

\draw[{Rays[]}-{Rays[]},dotted] (6.5,2.06)--(6.5,-0.06) 
node[below]  {$B_5=1$};


\draw[{Rays[]}-{Circle[open]},dotted] (7.5,2.06)--(7.5,1.3) 
node[below, fill=white] {$B_6=0$};

\end{tikzpicture}
  \end{center}

 
  The concept of thinning is particularly useful to analyze queueing
  networks. Suppose the departure stream of a machine is split into
  two substreams, e.g., a fraction $p$ of the jobs move on another
  machine and the rest ($1-p$) of the jobs leave the system. Then we
  can model the arrival stream at the second machine as a thinned
  stream (with probability $p$) of the departures of the first
  machine.

  Show that the Poisson process obtained by thinning the original
  process is $\sim P(\lambda t p)$ for any~$t$.

\hint{ Condition on the total number of arrivals $N(t)=m$ up to time
      $t$. Realize that the probability that a job is of type 1 is
      Bernoulli distributed, hence when you consider $m$ jobs in
      total, the number of type 1 jobs is binomially distributed.

      Again use that if the set $A$ can be split into disjoint sets
      $B_i$, i.e, $A=\bigcup_{i=1}^n B_i$, then
    \begin{equation*}
      \P{A} = \sum_{i=1}^n \P{A\given B_i} \P{B_i}.
    \end{equation*}
Now choose practical sets $B_i$. 


You might also consider the random variable 
  \begin{equation*}
    Y = \sum_{i=1}^N Z_i,
  \end{equation*}
  with $N\sim P(\lambda)$ and $Z_i\sim B(p)$. Show that the moment
  generating function of $Y$ is equal to the moment generating
  function of a Poisson random variable with parameter $\lambda p$.

}

    \begin{solution}
Suppose that  $N_1$ is the  thinned stream, and $N$ the total stream. Then
\begin{equation*}
  \begin{split}
    \P{N_1 = k}
&= \sum_{n=k}^\infty \P{N_1 =k, N = n} 
= \sum_{n=k}^\infty \P{N_1 =k\given N = n}\P{N=n} \\
&= \sum_{n=k}^\infty \P{N_1 =k\given N = n}e^{-\lambda} \frac{\lambda^n}{n!}
= \sum_{n=k}^\infty {n \choose k} p^k (1-p)^{n-k} e^{-\lambda} \frac{\lambda^n}{n!}\\
&= e^{-\lambda}\sum_{n=k}^\infty  \frac{p^k (1-p)^{n-k}}{k! (n-k)!} \lambda^n
= e^{-\lambda} \frac{(\lambda p)^k}{k!} \sum_{n=k}^\infty  \frac{(\lambda (1-p))^{n-k}}{(n-k)!}\\
&= e^{-\lambda} \frac{(\lambda p)^k}{k!} \sum_{n=0}^\infty  \frac{(\lambda (1-p))^{n}}{n!}
= e^{-\lambda} \frac{(\lambda p)^k}{k!} e^{\lambda(1-p)} \\
&= e^{-\lambda p} \frac{(\lambda p)^k}{k!}.
  \end{split}
\end{equation*}
We see that the thinned stream is Poisson with parameter $\lambda p$. (For notational ease, we left out the $t$, otherwise it is $P(\lambda t p)$. 

Now consider $Y=\sum_{i=1}^N Z_i$. Suppose that $N=n$, so that $n$
arrivals occurred. Then we throw $n$ coins with success probability
$p$. It follows that $Y$ is indeed a thinned Poisson random variable.
Model the coins as a generic Bernoulli distributed random variable
$Z$.  We first need
\begin{equation*}
  \E{e^{s Z}} = e^0 \P{Z=0} + e^{s} \P{Z=1} = (1-p) + e^s p.
\end{equation*}
Suppose that $N=n$, then since the $Z_i$ are i.i.d.,
\begin{equation*}
\E{e^{s\sum_{i=1}^n Z_i}} = \left(\E{e^{s Z}}\right)^n = \left(1 + p (e^s - 1)\right)^n
\end{equation*}
Then, using conditioning on $N$, 
\begin{equation*}
  \begin{split}
  \E{e^{s Y}}
&= \E{\E{e^{s\sum_{i=1}^n Z_i} \given N=n }} 
 = \E{\E{\left(1+p(e^s-1)\right)^n \given N=n }} \\
&= \sum_{n=0}^\infty \left(1+p(e^s-1)\right)^n e^{-\lambda} \frac{\lambda^n}{n!}
= e^{-\lambda} \sum_{n=0}^\infty \frac{\left(1+p(e^s-1)\right)^n \lambda^n}{n!}\\
&= e^{-\lambda} e^{\lambda (1+p(e^s-1))} = e^{\lambda p (e^s - 1)}.
  \end{split}
\end{equation*}
Thus, $Y$ has the same moment generating function as a Poisson
distributed random variable with parameter $\lambda p$. Since
moment-generating functions specify the distribution uniquely,
$Y\sim P(\lambda p)$.

\end{solution}
\end{question}    

\begin{question}
  Show that $\E{N(t)} = \lambda t$ and $\V{N(t)} = \lambda t$. Why is
  the SCV of $N(t)$ equal to $1/\lambda t$? Conclude that the relative
  variability of $N(t)$ becomes smaller as $t$ becomes larger. Observe
  that the SCV of a Poisson distributed random variable is not the
  same as the SCV of an exponentially distributed random variable.

  \hint{You might use generating functions here.}

  \begin{solution} I expect that this is easy for you by now. The
    exercise is just meant to help you recall this.

    When a random variable $N$ is Poisson distributed with parameter
    $\lambda$,
    \begin{equation*}
      \begin{split}
      \E N 
&= \sum_{n=0}^\infty n e^{-\lambda}\frac{\lambda^n}{n!}  \\
&= \sum_{n=1}^\infty n e^{-\lambda}\frac{\lambda^n}{n!}, \text{ since the term with $n=0$ cannot contribute} \\
&= e^{-\lambda} \lambda \sum_{n=1}^\infty \frac{\lambda^{n-1}}{(n-1)!} \\
&= e^{-\lambda} \lambda \sum_{n=0}^\infty \frac{\lambda^{n}}{n!}, \text{ by a change of variable}\\
&= e^{-\lambda} \lambda e^{\lambda} \\
&= \lambda.
      \end{split}
    \end{equation*}
Similarly, using that $\V N = \E{N^2} - (\E N)^2$, 
    \begin{equation*}
      \begin{split}
      \E{N^2}
&= \sum_{n=0}^\infty n^2 e^{-\lambda}\frac{\lambda^n}{n!}  \\
&= e^{-\lambda} \sum_{n=1}^\infty n \frac{\lambda^n}{(n-1)!}  \\
&= e^{-\lambda} \sum_{n=0}^\infty (n+1) \frac{\lambda^{n+1}}{n!}  \\
&= e^{-\lambda} \lambda \sum_{n=0}^\infty n \frac{\lambda^{n}}{n!}  +e^{-\lambda}\lambda \sum_{n=0}^\infty\frac{\lambda^{n}}{n!}  \\
&= \lambda^2  + \lambda.
\end{split}
\end{equation*}

The Poisson \emph{process} $\{N(t)\}$ is a much more complicated
object than the Poisson distributed random variable $N(t)$. The
process contains it is an \emph{ uncountable set} of random variables,
whereas $N(t)$ is just \emph{one} random variable. However, for a
fixed $t$ the element $N(t)$ is a random variable that is Poisson
distributed with parameter $\lambda t$. Using the above, the answer of
the question follows immediately.


With generating functions we get the result without too much
effort. Suppose $N$ is a Poisson distributed random variable. Then 
\begin{equation*}
  \begin{split}
\phi(z)
&=  \E{z^N} \\
&= \sum_{k=0}^\infty z^k \P{N=k} \\
&= \sum_{k=0}^\infty z^k \frac{\lambda^k}{k!} e^{-\lambda}  \\
&= e^{-\lambda} \sum_{k=0}^\infty  \frac{(z\lambda^k}{k!}  \\
&= e^{-\lambda} e^{z \lambda} \\
&= e^{(z-1)\lambda}.
  \end{split}
\end{equation*}
Observe that $\phi'(z) = \E{N z^N}$, so that $\phi'(1)=\E{N}$; also
$\phi''(z)=\E{N(N-1)z^{N-2}}$, hence $\phi''(1)=\E{N(N-1)}$. With this,
  \begin{align*}
    \E N &= \phi'(1)= \lambda,\\
\E{N^2} &= \phi''(1) + \E{N} = \lambda^2 + \lambda,\\
\V N &= \E{N^2}-(\E N)^2 = \lambda,\\
SCV &= \frac{\V{N(t)}}{(\E{N(t)})^2} = \frac{\lambda t}{(\lambda t)^2} = \frac1{(\lambda t)^2}.
  \end{align*}
Clearly, as $t$
increases, $1/\lambda t$ decreases.

There is point of confusion for some students. The SCV of an
exponentially distributed random variable is 1; the SCV of the related
Poisson process $N_\lambda(t)$ is \emph{not} identically~1 for all
$t$.

\end{solution}
\end{question}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "book"
%%% End:
