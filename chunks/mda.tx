
\section{MDA Algorithm}
\label{sec:mda}

\subsection*{Theory and Exercises}

\Opensolutionfile{hint}
\Opensolutionfile{ans}

A relatively easy way to obtain insight into the procedure is by
comparing this case to the single-server single station case.  Recall that, for the latter case, 
\begin{equation*}
  \E{W_Q} = \rho \E{S_r\given S_r>0} + \E{L_Q} \E S.
\end{equation*}
The first term is the probability that the server is found busy by an
arriving job times the expected remaining service time of the job in
service; the second is the expected time to clear the queue. 

For the closed queueing network, the probability to find $k$ at
station~$j$ is $p_j(k|n-1)$, where we use the arrival theorem that
states when $n$ jobs are present in the network, a `jumping' job sees
the stationary distribution of the same network but with one job
less. 

To wait in queue at station $j$ it is necessary that $k\geq c_j$, for
otherwise service can start right at the arrival moment. Thus, the
probability that all servers are busy is
\begin{equation*}
  \sum_{k=c_j}^{n-1} p_j(k|n-1).
\end{equation*}
(Compare the first $\rho$ in the above equation for $\E{W_Q}$).  The
service of the queue can start once the first job currently in service
leaves. This takes $1/c\mu_j$. (Compare this to $\E{S_r\given S_r>0}$.) 

The expected to clear the queue is $\E{L_{Q,j}}$ divided by the rate
at which jobs are served. Thus, this time must be $\E{L_{Q,j}}/c\mu_j$. Now,
\begin{equation*}
  \E{L_{Q,j}} = \sum_{k=c_j}^{n-1} (k-c_j)p_j(k|n-1).
\end{equation*}

All in all, the total average time at station $j$ is the average time
in queue plus the average service time of the job itself:
\begin{equation*}
  \E{W_j(n)} = \frac1{c_j \mu_j} \sum_{k=c_j}^{n-1} p_j(k|n-1) 
+ \frac{1}{c_j \mu_j} \sum_{k=c_j}^{n-1} (k-c_j) p_j(k|n-1) + \frac{1}{\mu_j},
\end{equation*}

Up- and down-crossing of level $k$ gives that
\begin{equation}
  \TH_j(n) p(k-1|n-1) = \min(k,c_j) p(k|n). 
\end{equation}

\begin{exercise}
  Apply the MDA algorithm to the network discussed in Exercise~\ref{ex:mva_numerical}, but now assume that station 1 has 2 servers. 
  \begin{hint}
 Just follow the description of the algorithm of the book of
    Zijm. I expect that you have read the solution of
    Exercise~\ref{ex:mva_numerical}.
  \end{hint}
  \begin{solution}
    Take $n=1$.
    \begin{align*}
      \E{W_0(1)} 
&= \sum_{k=c_0}^{n-1} \frac{k-c_0+1}{c_0 \mu_0} p_0(k|1-1) + \frac1{\mu_0} \\
&= \sum_{k=1}^{0} \frac{k-c_0+1}{c_0 \mu_0} p_0(k|1-1) + \frac1{\mu_0} \\
&= \frac1{\mu_0} \\
&= 2, \\
      \E{W_1(1)}
&= \sum_{k=c_1}^{n-1} \frac{k-c_1+1}{c_1 \mu_1} p_1(k|1-1) + \frac1{\mu_1} \\
&= \sum_{k=2}^{0} \frac{k-c_1+1}{c_1 \mu_1} p_1(k|1-1) + \frac1{\mu_1} \\
&= \frac1{\mu_1} \\
&= 3, \\
\E{W(1)} &= \sum_{i=0}^1 V_i \E W_i(1) = V_0 2 + V_1  3 = 1\cdot 2 + 2 \cdot 3 = 8\\
\TH_0(1) &= \frac{1}{\E{W(1)}} = 1/8, \\
\TH_1(1) &= V_1 \TH_0(1) = 2/8=1/4, \\
p_0(1|1) 
&= \TH_0(1)p_0(0|0)\E{S_0} \frac1{\min\{c_0,1)} = \frac18\cdot 1\cdot 2 \cdot \frac{1}{1}=1/4, \\ 
p_1(1|1) 
&= \TH_1(1)p_1(0|0)\E{S_1} \frac1{\min\{c_1,1)} = \frac14\cdot 1\cdot 3 \cdot \frac{1}{1}=3/4, \\
p_0(0|1) &= 1-p_0(1|1) = 3/4, \\
p_1(0|1) &= 1-p_1(1|1) = 1/4, \\
    \end{align*}
Now for $n=2$.
    \begin{align*}
      \E{W_0(2)}
&= \sum_{k=c_0}^{n-1} \frac{k-c_0+1}{c_0 \mu_0} p_0(k|2-1) + \frac1{\mu_0} \\
&= \sum_{k=1}^{1} \frac{k-c_0+1}{c_0 \mu_0} p_0(k|1) + \frac1{\mu_0} \\
&= \sum_{k=1}^{1} \frac{k-c_0+1}{c_0 }\E{S_0} p_0(k|1) + \frac1{\mu_0} \\
&=  \frac{1-1+1}{1 }2 p_0(1|1) + 2 \\
&=  2\cdot 1/4 + 2 =5/2\\
      \E{W_1(2)}
&= \sum_{k=2}^{2-1} \frac{k-c_1+1}{c_1 \mu_1} p_1(k|1-1) + \frac1{\mu_1} \\
&= \frac1{\mu_1} = 3, \\
\E{W(2)} &= \sum_{i=0}^1 V_i \E{W_i(2)} =  1\cdot 5/2 + 2 \cdot 3 = 5/2 + 6 = 17/2\\
\TH_0(2) &= \frac{2}{\E{W(2)}} = 2\cdot2/17=4/17, \\
\TH_1(2) &= V_1 \TH_0(2) = 2\cdot4/17=8/17, \\
p_0(1|2) 
&= \TH_0(2)p_0(0|1)\E{S_0} \frac1{\min\{c_0,1)} \\
&= 4/17\cdot 3/4 \cdot 2 \cdot 1\\
p_0(2|2) 
&= \TH_0(2)p_0(1|1)\E{S_0} \frac1{\min\{c_0,1)} \\
&= 2/17\cdot 1/4 \cdot 2 \cdot 1\\
p_0(0|2) &= 1 - p_0(1|2) - p_0(2|2) \\
p_1(1|2) 
&= \TH_1(2)p_1(0|1)\E{S_1} \frac1{\min\{c_1,1)} \\
&= 8/17 \cdot 1/4\cdot 3 \cdot 1/1 \\
p_1(2|2) 
&= \TH_1(2)p_1(1|1)\E{S_1} \frac1{\min\{c_1,2)} \\
&= 8/17 \cdot 3/4 \cdot 3 \cdot1/2\\
p_1(0|2) &= 1 - p_1(1|2) - p_1(2|2).
    \end{align*}
    I am done (and fed up). Its time to for the computer to take over
    \ldots
  \end{solution}
\end{exercise}


\begin{comment}
\begin{exercise}[use=false]
  \begin{enumerate}
  \item 
  Provide an interpretation of a single-server queueing server with a
  finite calling population in terms of a closed network.
\item Apply the MDA algorithm to this situation and show that the
  results of Exercise~\ref{ex:calling} come out.
\end{enumerate}
\begin{solution}
  \begin{enumerate}
  \item 
    Consider a closed-queueing network with two stations. Station 1 is
    a single-server station with exponentially distributed service
    times with mean $\mu$; station 2 has $N$ parallel exponential
    servers, each working at rate $\lambda$. If station 1 contains $n$
    jobs, then Station 2 contains $N-n$ jobs.  The rate at which jobs
    move from Station 2 to Station 1 is $(N-n)\lambda$, since $N-n$ of
    the $N$ servers of Station 2 are occupied. Jobs move from Station
    1 to Station 2 at rate $\mu$, provided $n\geq1$.
  \item     TBD.
  \end{enumerate}
  \end{solution}
\end{exercise}
\end{comment}


\begin{exercise}
Zijm.Ex.3.1.15
\begin{solution}
  There is no end to the number of variations we can make. An
  interesting case is to study the effect of increasing the capacity
  of the second slowest station. When the total number of jobs in the
  network is large, nearly all of them will wait in the queue in front
  of the bottleneck, here station 0. If, however, the number is not
  large, it may be interesting to increase the rate of the second
  slowest station (if that is easy or cheap compared to increasing the
  production rate of the bottleneck). Then the queue at the second
  slowest station will become smaller (at least, that is what I
  expect), hence the queue in front of the bottleneck must increase (I
  don't expect that the jobs from the second slowest will move to the
  faster stations.)  But then the fraction of idle time at the
  bottleneck must go down, hence the throughput must increase. Note
  that, if the throughput increases, the arrival rate at the other
  stations must increase, hence, the queue lengths at these stations
  must also increase. Thus effects runs counter the other effect
  (increasing the queue at the bottleneck.)

  So lets try.

First the old results again:


\begin{lstlisting}
from functools import lru_cache
import numpy as np
from numpy.linalg import eig


c = np.array([1, 1, 1, 1, 1])  # single server stations
P = np.matrix([
    [0, 1, 0, 0, 0],
    [0, 0, 1, 0, 0],
    [0, 0, 0, 1, 0],
    [0, 0, 0, 0, 1],
    [1, 0, 0, 0, 0],
]
)

v, w = eig(P.T)
x = v.real.argmax()
V = w[:, x].real
V = V / V[0]

mu = np.array([2, 2.5, 3, 3, 3])

@lru_cache(maxsize=None)
def EL(j, n):
    if n <= 0:
        return 0
    else:
        return TH(j, n) * EW(j, n)

@lru_cache(maxsize=None)
def EW(j, n):
    return (EL(j, n - 1) + 1) / mu[j]


@lru_cache(maxsize=None)
def TH(j, n):
    if j == 0:
        return n / sum(V[j] * EW(j, n) for j in range(len(mu)))
    else:
        return V[j] * TH(0, n)

print(" n  EL0  EL1  EL2   EL3   EL4 TH0")
for n in range(8, 11):
    res = "{:3d}".format(n)
    for j in range(len(mu)):
        res += "{:6.2f}".format(float(EL(j, n)))
    res += "{:6.2f}".format(float(TH(0,n)))
    print(res)
\end{lstlisting}\begin{lstlisting}
n  EL0  EL1  EL2   EL3   EL4 TH0
  8  2.89  1.67  1.15  1.15  1.15  1.69
  9  3.39  1.86  1.25  1.25  1.25  1.74
 10  3.93  2.05  1.34  1.34  1.34  1.79
\end{lstlisting}

Now we need to clear the cache, and redo the computations
The visit ratios do not change, only the processing rates.


\begin{lstlisting}
EL.cache_clear()
EW.cache_clear()
TH.cache_clear()

mu = np.array([2, 3, 3, 3, 3])

print(" n  EL0  EL1  EL2   EL3   EL4 TH0")
for n in range(8, 11):
    res = "{:3d}".format(n)
    for j in range(len(mu)):
        res += "{:6.2f}".format(float(EL(j, n)))
    res += "{:6.2f}".format(float(TH(0,n)))
    print(res)
\end{lstlisting}\begin{lstlisting}
n  EL0  EL1  EL2   EL3   EL4 TH0
  8  3.14  1.21  1.21  1.21  1.21  1.74
  9  3.71  1.32  1.32  1.32  1.32  1.79
 10  4.32  1.42  1.42  1.42  1.42  1.83
\end{lstlisting}

All is according to intuition. Indeed, the throughput increases
slightly. Thus, it may be interesting to increase the production rate
of non-bottleneck machines, provided its (much) easier/cheaper than
improving the bottleneck.
\end{solution}
\end{exercise}

\begin{exercise}
  Consider Example 3.5. Suppose you would add an extra machine to
  station 0. What will become the throughput and the average number of
  jobs at each station? Use the Marginal Distribution Analysis
  Algorithm. Plot also the distribution of the number of jobs in
  stations 0 and 1.
  \begin{solution}
    I first implement the algorithm. Then I use the case, i.e., the
    data of Example 3.4, and compare, as a test, the results to the
    results of the previous exercise.


\begin{lstlisting}
from functools import lru_cache
import numpy as np
from numpy.linalg import eig

mu = np.array([2, 2.5, 3, 3, 3])
c = np.array([1, 1, 1, 1, 1])  # single server stations
P = np.matrix([
    [0, 1, 0, 0, 0],
    [0, 0, 1, 0, 0],
    [0, 0, 0, 1, 0],
    [0, 0, 0, 0, 1],
    [1, 0, 0, 0, 0],
]
)

v, w = eig(P.T)
x = v.real.argmax()
V = w[:, x].real
V = V / V[0]


@lru_cache(maxsize=None)
def p(j, k, n):
    if n == 0:
        return 1
    if k > 0:
        return TH(j, n) * p(j, k - 1, n - 1) / min(c[j], k) / mu[j]
    else:
        return 1 - sum(p(j, k, n) for k in range(1, n + 1))


@lru_cache(maxsize=None)
def EW(j, n):
    res = sum((k - c[j] + 1) / c[j] / mu[j] * p(j, k, n - 1)
              for k in range(c[j], n))
    res += 1 / mu[j]
    return res


@lru_cache(maxsize=None)
def TH(j, n):
    if j == 0:
        return n / sum(V[j] * EW(j, n) for j in range(len(c)))
    else:
        return V[j] * TH(0, n)

print(TH(0, 10))
\end{lstlisting}\begin{lstlisting}
[[ 1.78901276]]
\end{lstlisting}

This is the same as what we see earlier. I guess all is ok. 

Now add a machine to station 0. Don't forget to clear the cache!


\begin{lstlisting}
p.cache_clear()
EW.cache_clear()
TH.cache_clear()

c = np.array([2, 1, 1, 1, 1])

n = 10
TH(0, n)
\end{lstlisting}\begin{lstlisting}
matrix([[ 2.08301549]])
\end{lstlisting}
\begin{lstlisting}
n = 30
TH(0, n)
\end{lstlisting}\begin{lstlisting}
matrix([[ 2.46519749]])
\end{lstlisting}

As expected, station 1 with processing rate $\mu_1=2.5$ now determines
the throughput of the network.



\begin{lstlisting}
def ELQ(j, n):
    return sum((k - c[j]) * p(j, k, n) for k in range(c[j], n + 1))


def EL(j, n):
    return sum(k * p(j, k, n) for k in range(n + 1))

print(EL(0, n))
print(TH(0, n) * EW(0, n)) # Little's law as a check.
\end{lstlisting}\begin{lstlisting}
[[ 1.9724301]]
[[ 1.9724301]]
\end{lstlisting}

Finally, plot the probability mass functions for stations 0, 1, and 2.

  

\begin{lstlisting}
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 2))
plt.plot([float(p(0, k, n)) for k in range(n + 1)], label="S0")
plt.plot([float(p(1, k, n)) for k in range(n + 1)], label="S1")
plt.plot([float(p(2, k, n)) for k in range(n + 1)], label="S2")
plt.legend()
filename = "figures/mda_mass.pdf"
plt.savefig(filename)
plt.close()
\end{lstlisting}

\includegraphics{mda_mass}

  \end{solution}
\end{exercise}





\Closesolutionfile{hint}
\Closesolutionfile{ans}
\subsection*{Hints}
\input{hint}
\subsection*{Solutions}
\input{ans}
\clearpage
