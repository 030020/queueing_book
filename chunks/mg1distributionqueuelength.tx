\section
[$M/G/1$ Queue Length Distribution]
{$\mathbf{M/G/1}$ Queue Length Distribution}
\label{sec:distr-queue-length}

In Section~\ref{sec:mg1} we derived the Pollackzek-Khintchine
Eq.~\eqref{eq:710} by which we can compute the average waiting time in
queue for the $M/G/1$ queue. Then, with Little's law, we therefore
also have the average queue length. However, if we need the loss
probability $\P{L>n}$, we need expressions for the stationary
distribution of the number of jobs in the system $p(n)=\P{L=n}$.  Here
we present a set of recursions by which we can compute $p(n)$ for
a given arrival rate $\lambda$ and service distribution~$F$ of the
service times. 

To derive the stationary distribution $p(n)$ we work in steps, and use
similar level-crossing arguments as in
Section~\ref{sec:batch-arrivals}.  However, we cannot simply copy the
derivation of the $M^X/M/1$ queue to the $M/G/1$ queue. To see this,
note that in the $M^X/M/1$ queue, the service times are exponential,
hence memoryless. For this reason, the service times `restart from
anew' at any moment in time, in particular at arrival moments. For the
$M/G/1$ queue, service times are, in general, not exponential, hence
not memoryless, hence do not restart at arrival times. For this reason
we need to focus on moments in time in which the system `restarts'. As
we will see, the argumentation is quite subtle, as it uses an
interplay of the PASTA property and the relation of Eq.~\eqref{eq:39}
between $\pi(n)$, $p(n)$ and $\delta(n)$. The important idea is to
focus on the \emph{departure} moments, as these moments can be
considered as restarts of the service times, and on the number of
arrivals $Y_k$ during the service time of the $k$th job.  Assume that
$f(j) = \P{Y_k = j}$ is given and write $G(j) = \P{Y_k > j}$.

First we concentrate on a downcrossing of level $n$, see
Figure~\ref{fig:mg1_2}; recall that level $n$ lies between states $n$
and $n+1$.  Suppose now that job `$k-1$' leaves $n+1$ jobs behind
after its service completion. Precisely when job $k$ leaves $n$ jobs
behind a downcrossing occurs, i.e., when
$\1{L(D_{k-1}) = n+1}\1{L(D_k)=n} = 1$. Let us write this in another
way. Observe that if $L(D_{k-1})=n+1$ and no other jobs arrive during
the service time $S_k$ of job $k$, i.e., when $Y_k=0$, it must also be
that job $k$ leaves $n$ jobs behind. If, however, $Y_k>0$, then
$L(D_k)\geq n+1$.  Thus, we see that
\begin{equation*}
  \1{L(D_{k-1}) = n+1}\1{L(D_k)=n} =  \1{L(D_{k-1}) = n+1}\1{Y_k=0}.
\end{equation*}
Consequently, the number of downcrossings up to time $t$ is
\begin{equation*}
  \begin{split}
  D(n+1, 0, t) 
%&= \sum_{k=1}^\infty \1{D_{k}\leq t}\1{L(D_{k})=n} \\
&= \sum_{k=1}^\infty \1{D_{k}\leq t}\1{L(D_{k-1})=n+1}\1{Y_k=0}.
  \end{split}
\end{equation*}
By using the definitions and limits developed in
Sections~\ref{sec:rate-stability} and~\ref{sec:poisson-arrivals-see},
\begin{equation*}
  \begin{split}
  \lim_{t\to\infty} \frac{  D(n+1, 0, t)}t 
&=   \lim_{t\to\infty}  \frac{D(t)}t \frac{D(n+1, t)}{D(t)}\frac{ D(n+1, 0, t)}{D(n+1,t)} \\
&=   \gamma \delta(n+1) \lim_{t\to\infty} \frac{ D(n+1, 0, t)}{D(n+1,t)} \\
&=   \gamma \delta(n+1) \P{Y=0}\\
& = \gamma \delta(n+1) f(0),
  \end{split}
\end{equation*}
where the last limit follows from the independence of $Y_k$ and
$L(D_{k-1})$, compare the derivation of~\eqref{eq:1332}.

\begin{figure}[tb]
  \centering
                  
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
                    semithick]
\node[state] (0) {$\delta(0)$}; 
\node[state] (1) [right of=0] {$\delta(1)$}; 
\node[state] (2) [right of=1] {$\delta(2)$}; 
\node[state] (3) [right of=2] {$\delta(3)$}; 
\node[state] (4) [right of=3] {$\delta(4)$}; 
%\node[state] (5) [right of=4] {$\cdots$}; 
\node (5) [above of=4] {};

\path 
(0) edge [bend left] node[above, very near start, fill=white] {$G(3)$} (5)
(0) edge [loop below] node[below, midway, fill=white] {$f(0)$} (0)
(1) edge [bend left] node[above, very near start, fill=white] {$G(3)$} (5)
(1) edge [loop below] node[below, midway, fill=white] {$f(1)$} (1)
(1) edge [bend left] node[below, midway, fill=white] {$f(0)$} (0)
(2) edge [bend left] node[above, very near start, fill=white] {$G(2)$} (5)
(2) edge [bend left] node[below, midway, fill=white] {$f(0)$} (1)
(2) edge [loop below] node[below, midway, fill=white] {$f(1)$} (2)
(3) edge [bend left] node[above, very near start, fill=white] {$G(1)$} (5)
(3) edge [loop below] node[below, midway, fill=white] {$f(1)$} (3)
(3) edge [bend left] node[below, midway, fill=white] {$f(0)$} (2)
(4) edge [bend left] node[below, near start] {$f(0)$} (3);

% \node[circ, right=of n-2] (n-1) {$n-1$}
% edge[loop below, thick]  node[midway, fill=white] {$\lambda f(0)$} (n-1); 

\draw[-, dotted, gray] (9.7,-1)--(9.7, 4.0) node[below, black] {level $3$};


\end{tikzpicture}
\caption{Level $3$ is crossed from below with rate
  $\gamma\delta(0)G(3) + \gamma\delta(1)G(3) + \cdots \gamma \delta(3) G(1)$ and crossed
  from above with rate $\gamma\delta(4) f(0)$. }
\label{fig:mg1_2}
\end{figure}


For the upcrossings, suppose first that $L(D_{k-1})=n>0$ and observe
that an upcrossing whenever $\1{L(D_{k-1})=n} \1{L(D_{k})>n} = 1$.
Again, we can convert this into a statement with the number of
arrivals during the service time $S_k$ of job $k$.  If $Y_k=0$, then
job $k$ must leave $n-1$ jobs behind, so no upcrossing can
happen. Next, if $Y_k=1$, then job $k$ leaves $n$ jobs behind, so
still no upcrossing occurs. In fact, level $n$ is upcrossed when iff
more than one job arrives during the service of job $k$, thus,
\begin{equation*}
\1{L(D_{k-1})=n} \1{L(D_{k})>n} = \1{L(D_{k-1})=n} \1{Y_{k}>1}.
\end{equation*}
More generally, level $n$ is upcrossed for $0<m\leq n$ whenever
\begin{equation*}
\1{L(D_{k-1})=m} \1{L(D_{k})>n} = \1{L(D_{k-1})=m} \1{Y_{k}>n-m+1},
\end{equation*}
while if $m=0$ (think about this),
\begin{equation*}
\1{L(D_{k-1})=0} \1{L(D_{k})>n} = \1{L(D_{k-1})=0} \1{Y_{k}>n}.
\end{equation*}
Again we define proper counting functions (c.f., one of the exercises
below), divide, and take suitable limits to find for upcrossing rate
\begin{equation}\label{eq:555}
\gamma \delta(0) G(n) + \gamma \sum_{m=1}^n \delta(m) G(n-m+1).
\end{equation}

Finally, equating the downcrossing and upcrossing rates and dividing
by $\gamma$ gives
\begin{equation}\label{eq:72}
  f(0) \delta(n+1) = \delta(0) G(n) + \sum_{m=1}^{n} \delta(m) G(n+1-m).
\end{equation}
Noting that $\pi(n) = \delta(n)$, which follows from~\eqref{eq:39} and
the fact that the $M/G/1$ queue length process has one-step
transitions, we arrive at
\begin{equation*}
  f(0) \pi(n+1) = \pi(0) G(n) + \sum_{m=1}^{n} \pi(m) G(n+1-m).
\end{equation*}

Again, we obtain a recursion by which we can compute, iteratively, the
state probabilities.  Setting $\pi(0)=1$ at first, $\pi(1)$ follows
from the above recursion. Then, with $\pi(0)=1$ and $\pi(1)$ we find
$\pi(2)$, and so on, up to some limit $N$. Then, set
$G=\sum_{n=0}^N \pi(n)$ as the normalization constant. As a practical
approach to determine whether $N$ is sufficiently large, compute
$p(n)$ also for a couple for values above $N$. If these values
decrease when $n$ becomes larger and they are very small compared to
the $p(n)$ with $n<N$, $N$ is most surely sufficiently large. However,
getting formal bounds on a proper size of $N$ requires more work than
we can do here.

\begin{question}
Provide the details behind the derivation of Eq.~\ref{eq:555}.
\hint{Define for $m=1,\ldots, n$
\begin{equation*}
  D(m, n, t) = \sum_{k=1}^\infty \1{D_{k}\leq t}\1{L(D_{k-1})=m}\1{Y_k>n-m+1},
\end{equation*}
and 
\begin{equation*}
  D(0, n, t) = \sum_{k=1}^\infty \1{D_{k}\leq t}\1{L(D_{k-1})=0}\1{Y_k>n}.
\end{equation*}
Then, divide by $D(n,t)$ and $D(t)$ and take limits.}

\begin{solution}
 Define, for the first term,
\begin{equation*}
  D(0, n, t) = \sum_{k=1}^\infty \1{D_{k}\leq t}\1{L(D_{k-1})=0}\1{Y_k>n}
\end{equation*}
Then
\begin{equation*}
  \begin{split}
  \lim_{t\to\infty} \frac{  D(0, n, t)}t 
&=   \lim_{t\to\infty}  \frac{D(t)}t \frac{D(0, t)}{D(t)}\frac{ D(0, n, t)}{D(0,t)} \\
&=   \gamma \delta(0) \lim_{t\to\infty} \frac{ D(0, n, t)}{D(0,t)} \\
&=   \gamma \delta(0) \P{Y>n}\\
& = \gamma \delta(0) G(n).
  \end{split}
\end{equation*}
For the second term, define
\begin{equation*}
  D(m, n, t) = \sum_{k=1}^\infty \1{D_{k}\leq t}\1{L(D_{k-1})=m}\1{Y_k>n-m+1}
\end{equation*}
then
\begin{equation*}
  \begin{split}
  \lim_{t\to\infty} \frac{  D(m, n, t)}t 
&=   \lim_{t\to\infty}  \frac{D(t)}t \frac{D(m, t)}{D(t)}\frac{ D(m, n, t)}{D(m,t)} \\
&=   \gamma \delta(m) \lim_{t\to\infty} \frac{ D(m, n, t)}{D(m,t)} \\
&=   \gamma \delta(m) \P{Y>n-m+1}\\
& = \gamma \delta(m) G(n-m+1).
  \end{split}
\end{equation*}


\end{solution}
\end{question}


\begin{question}
  Suppose that for the $M/G/1$ queue, $L(D_{k-1})>0$. Why is
  $S_k = D_k - D_{k-1}$?  If $L(D_{k-1}) = 0$, the time between
  $D_{k-1}$ and $D_k$ is \emph{not} equal to $S_k$. What is then the
  distribution of $D_k-D_{k-1}$?
\hint{  Realize that if $L(D_{k-1})=0$, job $k-1$ leaves behind an empty
  system. Thus, before job $k$ can leave, it has to arrive. In other
  words, $D_{k-1}<A_k$.  Since job $k$ arrives to an empty system,
  his service starts right away, to that the time between $A_k$ and
  $D_k$ is equal to the service time of job $k$.}
\begin{solution}
  When $L(D_{k-1})>0$, job $k$ is already in the system when job $k-1$
  finishes its service and leaves. Thus, at the departure time
  $D_{k-1}$ of job $k-1$, the service of job $k$ can start right away
  at $D_{k-1}$. Then, $D_k=D_{k-1}+S_k$.


    When job $k-1$ leaves an empty system behind, $D_k= A_k + S_k$,
    since job $k$ sees an empty system, hence its service can start
    right away after its arrival time at time $A_k$. Since the arrival
    process is Poisson by assumption, the time to the next arrival
    after $D_{k-1}$ is exponentially distributed with rate
    $\lambda$. (Recall the memoryless property of the interarrival
    times.) Thus, $A_k - D_{k-1}$ has the same distribution as $X_k$,
    so that $\P{D_k - D_{k-1} \leq x} = \P{X_k + S_k\leq x}$. BTW: Can
    you simplify this expression by using conditioning on $X_k$?
\end{solution}
\end{question}

\begin{question}
 Explain that if the service time is $s$, then
\begin{equation}\label{eq:4}
  \P{Y_k = j\given S=s} = e^{-\lambda s}\frac{(\lambda s)^j}{j!}.
\end{equation}
\hint{ If $s$ is deterministic, the number of arrival during a fixed
    period of time with length $s$ must be Poisson distributed.}
  \begin{solution}
If the service time is $s$ units long, we need to know the number of arrivals during this interval. By assumption, the arrival process is Poisson. 
  \end{solution}
\end{question}

\begin{question}
 Explain that 
\begin{equation}\label{eq:10}
  \P{Y_k = j} = \int_0^\infty e^{-\lambda s}\frac{(\lambda s)^j}{j!}\, \d F(s).
\end{equation}
where $F$ is the distribution of the service times.
\begin{solution}
  We use a conditioning argument to arrive at this result. The
  probability that the service time is $s$ units long is written in
  various ways in the literature: $F(\d s) = \d F(s) = \P{S\in \d s}$.
  (To properly define this we need measure theory\ldots). When $F$ has
  a density $f$, then $\d F(s) = f(s) \d s$.  Note that when $S$ is
  discrete, it does not have density everywhere. With this,
    \begin{equation*}
    \P{Y_k=j} = \int_0^\infty \P{Y_k =j\given S=s}\P{S\in \d s} =
    \int_0^\infty \P{Y_k =j\given S=s} \d F(s).
    \end{equation*}
    Using the answer of the previous problem we arrive at the result.
\end{solution}
\end{question}

\begin{question}
  If $S$ is deterministic and equal to $s$, show that
  Eq.~\eqref{eq:10} reduces to Eq.~\eqref{eq:4}.  \hint{If $S=s$ then
    $\P{S=s}=1$, i.e., all probability mass lies at $s$. Thus, all
    arrivals must occur during $[0,s]$.}

  \begin{solution}
    If $\P{S=s}=1$, then $\int_0^\infty g(x) \d F(x) = g(s)$ for any
    sensible function $g$. More specifically, since $\P{S=s}=1$, the
    distribution $F$ has an atom at $s$. The size of the atom is
    $F(s)-F(s-)=1$. Thus, for instance,
    $\int_0^\infty x \d F(x) = s (F(s)-F(s-)) =s$. The function $x$ in
    this integral can be replaced by any function $g(x)$ (provided $g$
    satisfies some technical regularity conditions. If you are
    mathematically interested: it must be measurable.)  Applying this
    result to the function $e^{-\lambda x}(\lambda x)^j/j!$ we obtain
    the answer.
  \end{solution}
\end{question}

\begin{question}
 If $S\sim \exp(\mu)$, show that 
  \begin{equation} \label{eq:41}
f(j) = \P{Y_k = j} = \frac{\mu}{\lambda+\mu}\left(\frac{\lambda}{\lambda+\mu}\right)^j.
  \end{equation}
  \begin{solution}
Use conditional probability to see that 
\begin{equation*}
  \begin{split}
  \P{Y_n = j} 
&= \int_0^\infty e^{-\lambda s}\frac{(\lambda s)^j}{j!}\, \d F(s) = \int_0^\infty e^{-\lambda s}\frac{(\lambda s)^j}{j!} \mu e^{-\mu s}\, \d s \\
&= \frac{\mu}{j!}\lambda^j \int_0^\infty e^{-(\lambda+\mu) s}s^j\,\d s = \frac{\mu}{j!}\left(\frac{\lambda}{\lambda+\mu}\right)^j \int_0^\infty e^{-(\lambda+\mu) s}((\lambda+\mu)s)^j\,\d s \\
&= \frac{\mu}{j!}\left(\frac{\lambda}{\lambda+\mu}\right)^j \frac{j!}{\lambda+\mu}.
\end{split}
\end{equation*}
Here I also used the hints and  results of Exercise~\ref{ex:33}.

In hindsight, this result could have been derived in another way, in
fact by using the result of Exercise~\ref{ex:3} in which we analyzed a
merged Poisson process. Consider the Poisson process with rate
$\lambda+\mu$ that arises when the arrival and service process are
merged. The probability that an arrival corresponds to an epoch of the
merged process is $\lambda/(\lambda+\mu$ and the probability that a
departure corresponds to an epoch of the merged process is
$\mu/(\lambda+\mu)$.  The probability that $j$ arrivals occur before a
service occurs, is the same as the probability that a geometrically
distributed random variable with success probability
$\mu/(\lambda+\mu) = 1-p$ takes the value $j$.
  \end{solution}
\end{question}

\begin{question}
 If $S\sim \exp(\mu)$, show that 
  \begin{equation}
G(j) = \sum_{k=j+1}^\infty f(k) =  \left(\frac{\lambda}{\lambda+\mu}\right)^{j+1}.
  \end{equation}
\begin{solution}
  Take $\alpha = \lambda/(\lambda+\mu)$ so that
  $f(j) = (1-\alpha) \alpha^j$.
\begin{equation*}
  \begin{split}
  G(j) 
&= \sum_{k=j+1}^\infty f(k)  = (1-\alpha) \sum_{k=j+1}^\infty \alpha^k \\
& = (1-\alpha) \sum_{k=0}^\infty \alpha^{k+j+1}, \text{ by change of variable}\\
& = (1-\alpha) \sum_{k=0}^\infty \alpha^{k}\alpha^{j+1}= (1-\alpha)\alpha^{j+1} \sum_{k=0}^\infty \alpha^k \\
&= (1-\alpha)\alpha^{j+1} \frac{1}{1-\alpha} = \alpha^{j+1}.
  \end{split}
\end{equation*}
    \end{solution}
\end{question}

\begin{question}
  Use Eq.~\eqref{eq:41} to derive a closed-form expression for the
  queue length distribution for the $M/M/1$ queue.  \hint{ Define
    shorthands such as $\alpha=\lambda/(\lambda+\mu)$, so that
    $1-\alpha = \mu/(\lambda+\mu)$, and
    $\alpha/(1-\alpha) = \lambda /\mu = \rho$.  Then, with the
    previous exercise that $f(n) = \alpha^n(1-\alpha)$ and
    $G(n) = \alpha^{n+1}$.  (I did not get this result for
    `free'. Indeed, it's just algebra, but please try to derive this
    yourself. Its good to hone your computational skills.)}

    \begin{solution}
      Take $n=0$. Then $f(0) \pi(1) = (1-\alpha)\pi(1)$, and
      $\pi(0)G(0) = \pi(0)\alpha$. Thus,
      $\pi(1) = \pi(0)\alpha/(1-\alpha) = \rho \pi(0)$. 

For $n=1$,
\begin{equation*}
  \begin{split}
  (1-\alpha)  \pi(2) 
&= \pi(0)G(1) + \pi(1)G(1) = \pi(0)G(1)(1+\rho) \\
&= \pi(0)\alpha^2(1+\rho) = \pi(0)\alpha \alpha (1+\rho) = \pi(0)\alpha \rho.
  \end{split}
\end{equation*}
Dividing by $1-\alpha$, we get
\begin{equation*}
  \pi(2) = \pi(0)\rho^2
\end{equation*}

Finally, now that we suspect that $\pi(n) = \rho^n \pi(0)$, let's fill
it in, and see whether it checks. We divide both sides by $\pi(0)$ so
that we are left with checking that
\begin{equation*}
  \begin{split}
    (1-\alpha)\rho^{n+1} 
&= \alpha^{n+1} + \sum_{m=1}^n \rho^m \alpha^{n-m+2}  \\
&= \alpha^{n+1} + \alpha^{n+2}\sum_{m=1}^n (\rho/\alpha)^m  \\
&= \alpha^{n+1} + \alpha^{n+1}\rho \sum_{m=0}^{n-1} (\rho/\alpha)^m \\
&= \alpha^{n+1} + \alpha^{n+1}\rho \sum_{m=0}^{n-1} (\rho/\alpha)^m\\
&= \alpha^{n+1} + \alpha^{n+1}\rho \frac{1-(\rho/\alpha)^n}{1-\rho/\alpha)}\\
&= \alpha^{n+1} - \alpha^{n+1}(1-(\rho/\alpha)^n), \quad\text{as } \rho/\alpha = 1+\rho,\\
&= \alpha^{n+1}(\rho/\alpha)^n = \alpha \rho^n.\\
  \end{split}
\end{equation*}
Since $\rho=\alpha/(1-\alpha)$ we see that the left and right hand sides are the same. 

Thus we get that $\pi(n) = \delta(n) = (1-\rho) \pi(0)$; a result we
obtained earlier for the $M/M/1$ queue.
\end{solution}
\end{question}


\begin{question}
 With the above results we can compute
  Eq.~\eqref{eq:10} for deterministic service times or exponential
  service times. When, however, the service distribution $F$ is some
  general distribution function, we need numerical methods to evaluate
  Eq.~\eqref{eq:10}. Can you design a suitable numerical method for
  this problem?
  \begin{solution}
  A simple numerical method is as follows. Make a grid of
  size $\d x$, for some small number $\d x$, e.g. $\d x=1/100$, and write
  $f_i = \P{S\in(i\d x, (i+1)\d x]} = F((i+1)\d x) - F(i\d x)$. Then 
  \begin{equation*}
    \begin{split}
  \P{Y_k = j} 
&= \int_0^\infty e^{-\lambda s}\frac{(\lambda x)^j}{j!} \d F(x) 
\approx \sum_{i=1}^\infty e^{-\lambda i \d x}\frac{(\lambda i\d x)^j}{j!} f_i \d x
    \end{split}
\end{equation*}

Lets try a numerical experiment. Topics like these are, of course, not
necessary for the course, but I hope that you study it nonetheless.
You will have to use it for other courses later anyway.



\lstinputlisting{/home/nicky/Dropbox/nicky/vakken/qsim/queueing_book/chunks/mg1distributionqueuelength_1.tex}




\lstinputlisting{/home/nicky/Dropbox/nicky/vakken/qsim/queueing_book/chunks/mg1distributionqueuelength_2.tex}


Since I don't know when to stop the integral I just try a few values;
of course stopping the integration at $x=50$ is too small, since
$50\d x=50/100=1/2$, but I include it for illustrative
purposes. Stopping at $500$ seems ok, since the results of the last
two integrals are nearly the same. This also suggestst that
$\d x=1/100$ is sufficiently small. In general, however, one must take
care and try various values for $\d x$ and the integration limits.


In general, for more complicated situations is it best to use a
numerical library to compute the above integral. The methods have been
designed to produce good and reliable results, and, typically, it is
very hard to improve these methods. Thus, let's try a real number
cruncher.



\lstinputlisting{/home/nicky/Dropbox/nicky/vakken/qsim/queueing_book/chunks/mg1distributionqueuelength_3.tex}


Comparing these results to our simple estimates we see that they are
the same.
\end{solution}
\end{question}


\begin{question}[use=false]
  To analyze the distribution of $L$ of the $M/G/1$ we concentrated on
  the departure epochs. Why will this not help to find the
  distribution of $L$ for the $M/G/c$ queue?
  \begin{solution}
    \TBD.
  \end{solution}
\end{question}





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "book"
%%% End:
